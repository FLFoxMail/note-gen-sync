# 强化学习笔记

**强化学习的定义**：
强化学习（reinforcement learning，RL）讨论的问题是智能体（agent）如何在复杂、不确定的环境（environment）中最大化其能获得的奖励。强化学习由智能体和环境两部分组成，智能体在环境中获取某个状态后，会利用该状态输出一个动作（action），该动作在环境中执行后，环境会根据智能体采取的动作，输出下一个状态以及当前动作带来的奖励。智能体的目的是尽可能多地从环境中获取奖励。

**强化学习中的概念**：
- **动作（action）**：环境接收到的智能体基于当前状态的输出。
- **状态（state）**：智能体从环境中获取的状态。
- **奖励（reward）**：智能体从环境中获取的反馈信号，指定了智能体在某一步采取某个策略后是否得到奖励及奖励的大小。
- **探索（exploration）**：在当前情况下，继续尝试新的动作，有可能得到更高奖励，也可能一无所获。
- **利用（exploitation）**：在当前情况下，继续尝试已知的可获得最大奖励的过程，即选择重复执行当前动作。
- **深度强化学习（deep reinforcement learning）**：不需要手动设计特征，仅需输入状态就可让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。
- **全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）**：当智能体的状态与环境的状态等价时，称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，称这个环境是部分可观测的。
- **部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）**：马尔可夫决策过程的泛化，依然具有马尔可夫性质，但假设智能体无法感知环境的状态，只能知道部分观测值。
- **动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）**：在给定环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则称为连续动作空间。

**强化学习的核心问题**：
探索和利用是强化学习中的两个核心问题。探索是通过尝试不同动作来得到最佳策略（带来最大奖励的策略），利用是采取已知的可带来很大奖励的动作。在强化学习开始时，智能体只能通过试错来探索，以理解采取的动作能否带来好的奖励，同时也需要在探索和利用之间进行权衡，以牺牲一些短期奖励来学习到更好的策略。

**强化学习与监督学习的区别**：
- 强化学习输入的样本是序列数据，而监督学习的样本是独立的。
- 强化学习中智能体获得的观测并非独立同分布，是具有相关性的时间序列数据，且智能体无法立刻获得反馈，明确得知哪个动作是正确的；而监督学习中输入的数据应是没有关联的，且会告诉学习器正确的标签以便其修正预测。

**强化学习的分类**：
- **基于策略的（policy-based）**：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。
- **基于价值的（valued-based）**：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。
- **有模型（model-based）结构**：智能体通过学习状态的转移来进行决策。智能体根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习。当智能体知道状态转移函数`P(s_{t + 1}|s_{t},a_{t})`和奖励函数`R(s_{t},a_{t})`后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为有模型强化学习。
    -![有模型结构](https://cdn.jsdelivr.net/gh/FLFoxMail/note-gen-image-sync@main/9aa9f968-5b93-4fea-bbad-976866a9228c.png)
- **免模型（model-free）结构**：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用免模型强化学习。免模型强化学习智能体的模型里面没有环境转移的模型。
    -![免模型结构](https://cdn.jsdelivr.net/gh/FLFoxMail/note-gen-image-sync@main/9aa9f968-5b93-4fea-bbad-976866a9228c.png)

**强化学习智能体的组成成分和类型**：
对于一个强化学习智能体，它可能有一个或多个如下的组成成分：
- **策略（policy）**：智能体会用策略来选取下一步的动作。
- **价值函数（value function）**：用于对当前状态进行评估，评估智能体进入某个状态后对后面的奖励带来的影响，价值函数值越大，说明智能体进入这个状态越有利。
- **模型（model）**：表示智能体对环境的状态进行理解，决定环境中世界的运行方式。

**强化学习中的函数和算法**：
- **价值函数**：价值函数的值是对未来奖励的预测，用于评估状态的好坏。价值函数中有一个折扣因子，希望在尽可能短的时间内得到尽可能多的奖励。价值函数的定义为：
    - `V_π(s) = E_π[G_t | s_t = s] = E_π[∑_{k = 0}^{∞}γ^k r_{t + k + 1} | s_t = s]`，对于所有的`s ∈ S`
    - `Q_π(s,a) = E_π[G_t | s_t = s, a_t = a] = E_π[∑_{k = 0}^{∞}γ^k r_{t + k + 1} | s_t = s, a_t = a]`
- **策略**：策略是智能体的动作模型，决定智能体的动作。策略可分为随机性策略和确定性策略。
    - 随机性策略（stochastic policy）：`π(a|s) = p(a_t = a|s_t = s)`，输入一个状态`s`，输出一个概率，对这个概率分布进行采样，可得到智能体将采取的动作。
    - 确定性策略（deterministic policy）：`a* = argmax_a π(a | s)`，智能体直接采取最有可能的动作。
- **基于价值的强化学习算法**：有`Q`学习（`Q-learning`）、`Sarsa`等。
- **基于策略的强化学习算法**：有策略梯度（`Policy Gradient`，`PG`）算法等。
- **演员-评论员算法**：同时使用策略和价值评估来做出决策。智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，可在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

**强化学习中的其他问题**：
- **探索-利用窘境（exploration-exploitation dilemma）**：探索（估计摇臂的优劣）和利用（选择当前最优摇臂）这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方。想要累积奖励最大，必须在探索与利用之间达成较好的折中。
- **单步奖励最大化**：与监督学习任务不同，强化学习任务的最终奖励在多步动作之后才能观察到。即使在最大化单步奖励（仅考虑一步动作）的简单情形下，强化学习仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。然而，更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。
- **学习（learning）和规划（planning）**：学习和规划是序列决策的两个基本问题。在强化学习中，环境初始时是未知的，智能体通过不断地与环境交互，逐渐改进策略。
    -![价值函数/策略 思考 - 规划 动作 - 构建虚拟世界](https://cdn.jsdelivr.net/gh/FLFoxMail/note-gen-image-sync@main/d07261af-b189-45e2-810f-8520e9517280.png)
- **智能体的学习分类**：
    - **基于价值的智能体（value-based agent）**：显式地学习价值函数，隐式地学习其策略，策略是从学到的价值函数里面推算出来的。
    - **基于策略的智能体（policy-based agent）**：直接学习策略，给它一个状态，它会输出对应动作的概率，没有学习价值函数。
    - **演员-评论员智能体（actor-critic agent）**：把基于价值的智能体和基于策略的智能体结合起来，学习了策略和价值函数，通过两者的交互得到最佳的动作。
- **环境的观测和状态**：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，常用实值的向量、矩阵或者更高阶的张量来表示状态和观测。
- **强化学习中的权衡问题**：强化学习中一个重要的课题是近期奖励和远期奖励的权衡，研究如何让智能体取得更多的远期奖励。
- **强化学习的过程**：
    - 智能体在采取当前动作时会依赖于它之前得到的历史，可把整个游戏的状态看成关于这个历史的函数：`s_t = f(H_t)`。
    - 在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。历史是观测、动作、奖励的序列：`H_t = o_1, a_1, r_1,..., o_t, a_t, r_t`。
    - 当智能体的状态与环境的状态等价，即智能体能够观察到环境的所有状态时，这个环境是完全可观测的（fully observed），强化学习通常被建模成一个马尔可夫决策过程（Markov decision process，MDP）的问题。在马尔可夫决策过程中，`o_t = s_e t = s_a t`。
    - 但是当智能体只能看到部分的观测，这个环境是部分可观测的（partially observed）。在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）的问题。部分可观测马尔可夫决策过程可以用一个七元组描述：`(S, A, T, R, Ω, O, γ)`。其中`S`表示状态空间，为隐变量，`A`为动作空间，`T(s'|s, a)`为状态转移概率，`R`为奖励函数，`Ω(o|s,a)`为观测概率，`O`为观测空间，`γ`为折扣因子。
- **强化学习的特征**：
    - 强化学习会试错探索，通过探索环境来获取对环境的理解。
    - 强化学习智能体会从环境里面获得延迟的奖励。
    - 在强化学习的训练过程中，时间非常重要，得到的是有时间关联的数据（sequential data），而非独立同分布的数据。
    - 智能体的动作会影响它随后得到的数据，需要让智能体的动作一直稳定地提升。
- **深度强化学习**：早期的强化学习为标准强化学习，将强化学习与深度学习结合起来，形成了深度强化学习（deep reinforcement learning），即深度强化学习 = 深度学习 + 强化学习。
- **预演**：从当前帧对动作进行采样，生成很多局游戏。将当前的智能体与环境交互，会得到一系列观测。每一个观测可看成一个轨迹（trajectory）：`τ = (s_0, a_0, s_1, a_1,...)`。

**监督学习**：
监督学习假设拥有大量被标注的数据，且这些数据满足独立同分布。训练分类器（如神经网络）时，需将正确标签信息传递给它。当神经网络预测错误，便告知其正确标签。最后依据该错误写出损失函数，通过反向传播来训练神经网络。
    -![训练有标签的数据](https://cdn.jsdelivr.net/gh/FLFoxMail/note-gen-image-sync@main/d1ced0f1-aaba-4d0a-afdb-c67fc0519a62.png)

**强化学习示意**：
![智能体 - 状态 - 奖励 - 动作](https://cdn.jsdelivr.net/gh/FLFoxMail/note-gen-image-sync@main/151a782f-4ff9-43a7-b3d1-3a673ae59267.png) 强化学习中，智能体在环境里面获取某个状态后，利用该状态输出一个动作，这个动作在环境中执行后，环境会输出下一个状态以及当前动作带来的奖励。 